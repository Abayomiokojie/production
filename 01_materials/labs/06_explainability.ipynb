{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('SRC_DIR', '../../05_src'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "ft_path = os.getenv(\"CREDIT_DATA\", '/data/credit')\n",
    "df_raw = pd.read_csv(ft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 19:58:38,968, data.py, 22, INFO, Getting data from ../../05_src/data/credit/cs-training.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150000 entries, 0 to 149999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                Non-Null Count   Dtype  \n",
      "---  ------                                --------------   -----  \n",
      " 0   delinquency                           150000 non-null  float32\n",
      " 1   revolving_unsecured_line_utilization  150000 non-null  float32\n",
      " 2   age                                   150000 non-null  float32\n",
      " 3   num_30_59_days_late                   150000 non-null  float32\n",
      " 4   debt_ratio                            150000 non-null  float32\n",
      " 5   monthly_income                        120269 non-null  float32\n",
      " 6   num_open_credit_loans                 150000 non-null  float32\n",
      " 7   num_90_days_late                      150000 non-null  float32\n",
      " 8   num_real_estate_loans                 150000 non-null  float32\n",
      " 9   num_60_89_days_late                   150000 non-null  float32\n",
      " 10  num_dependents                        146076 non-null  float32\n",
      " 11  high_debt_ratio                       150000 non-null  float32\n",
      " 12  missing_monthly_income                150000 non-null  float32\n",
      " 13  missing_num_dependents                150000 non-null  float32\n",
      "dtypes: float32(14)\n",
      "memory usage: 8.0 MB\n"
     ]
    }
   ],
   "source": [
    "from credit.data import load_data\n",
    "\n",
    "X, Y = load_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Artifacts\n",
    "\n",
    "Previously, we built a procedure for hyperparameter tuning. The process produces parametrized models that use the optimal parameters based on Grid Search or Hyperopt. The models are saved in the object store. We can access these objects via MLFlow's API using the uri: `models:/<model name>/<model version>`\n",
    "\n",
    "Alternatively, the model could have been stored using the standard pickle format. We can load a model using `pickle.load()` within  a context manager that handles the file (a `with(open(f, 'rb'))...` statement). The context manager will help close files in case of unexpected termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "mlflow.set_tracking_uri('http://localhost:5001')\n",
    "model_name = 'CreditLogisticModel'\n",
    "model_version = '6'\n",
    "model_uri = f'models:/{model_name}/{model_version}'\n",
    "pipe = mlflow.sklearn.load_model(model_uri)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we loaded a logistic regression model. The model, however, could have been from another family of models (Random Forest, Neural Net, etc.)\n",
    "\n",
    "Below, we will explore some model-agnostic explainability methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial Dependence Plots\n",
    "\n",
    "+ [Partial Dependence Plots (PDP)](https://scikit-learn.org/stable/modules/partial_dependence.html) show the relationship between the target response and the input feature. \n",
    "+ They can be constructed for one or two inputs at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Why do we fit again?\n",
    "\n",
    "pipe.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Feature PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(pipe, X_train, \n",
    "                                        features = ['revolving_unsecured_line_utilization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(pipe, X_train, \n",
    "                                        features = ['revolving_unsecured_line_utilization', 'debt_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Feature PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(pipe, X_train, \n",
    "                                        features = [('revolving_unsecured_line_utilization', 'debt_ratio')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Values\n",
    "\n",
    "You may require the underlying data of the plots above. To obtain it, use the function [`partial_dependence()`](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.partial_dependence.html#sklearn.inspection.partial_dependence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "partial_dependence(pipe, X_train, features = ['revolving_unsecured_line_utilization', 'high_debt_ratio'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance\n",
    "\n",
    "+ Permutation feature importance measures the contribution of each feature to a fitted model's performance.\n",
    "+ Randomly shuffles the values of a single feature and observing the result degradation of the model's score. If shuffling a feature greatly degrades performance, then we say the feature is important.\n",
    "+ Shuffling is involved, therefore it is convenient (and costly) to perform several repetitions.\n",
    "\n",
    "[Scikit's Documentation](https://scikit-learn.org/stable/modules/permutation_importance.html) makes this warning:\n",
    "\n",
    "> **Warning**: Features that are deemed of **low importance for a bad model** (low cross-validation score) could be **very important for a good model**. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "pi_res = permutation_importance(\n",
    "    pipe, X_test, y_test, \n",
    "    n_repeats=30, \n",
    "    scoring = \"neg_log_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary with the following entries:\n",
    "\n",
    "+ `importances_mean`: mean of feature importance.\n",
    "+ `importances_std`: standard deviation of feature importance over n_repeats.\n",
    "+ `importances`: raw permutation importance scores (one feature per row, one reshuffle per column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_dt = pd.DataFrame(pi_res.importances).T\n",
    "importances_dt.columns = X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bp = plt.boxplot(importances_dt, vert = False,  labels = importances_dt.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        pd.Series(pi_res.importances_mean, index = X_test.columns, name = 'Mean Importance'),\n",
    "        pd.Series(pi_res.importances_std, index = X_test.columns, name = \"Std Importance\")\n",
    "    ], \n",
    "    axis = 1).sort_values(\"Mean Importance\", ascending  = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP Values\n",
    "\n",
    "\n",
    "+ SHAP is an advanced approach for providing explanation to model results. \n",
    "+ One library that implements this procedure is [Shap](https://shap.readthedocs.io/en/latest/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainers\n",
    "\n",
    "+ SHAP values can be calculated for any model, however, the procedure can be computationally expensive.\n",
    "+ For certain models, some specific functions exist to speed the cauclations: they are contained in the [`shap.explainers`](https://shap.readthedocs.io/en/latest/api.html#explainers) module. A few noteable functions are:\n",
    "\n",
    "    - [`shap.Explainer()`](https://shap.readthedocs.io/en/latest/generated/shap.Explainer.html#shap.Explainer): the primary explainer interface and chooses the explanation algorithm for you.\n",
    "    - [`shap.TreeExplainer()`](https://shap.readthedocs.io/en/latest/generated/shap.TreeExplainer.html#shap.TreeExplainer): implements Tree Shap, a procedure optimized for tree-based ensemble methods (Random Forest, XGBoost, etc.)\n",
    "    - [`shap.LinearExplainer()`](https://shap.readthedocs.io/en/latest/generated/shap.LinearExplainer.html#shap.LinearExplainer): computes SHAP Values for linear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining explanations from shap\n",
    "\n",
    "+ Shap can obtain local and global explanations. The model produces additive explanations, therefore, obtaining global explanations is equivalent to obtaining individual explanations for all samples.\n",
    "+ Shap can obtain explanations for testing and training samples.\n",
    "+ Local explanations are obtained as shap_values, which reflect the contribution of each feature to the prediction made for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the shap package\n",
    "\n",
    "+ SHAP works on classifiers, but our pipelines contain preprocessing and classification steps. \n",
    "+ We can access each individual step through the `.named_steps` attirbute.\n",
    "+ Notice that we apply the transformation (`ColumnTransformer`) step to obtain transformed data and store the results in `data_transform`.\n",
    "+ Feature names are obtained from the preprocessor's `.get_feature_names_out()` which exposes the names of the features after they have been transformed by the `ColumnTransformer`.\n",
    "+ The explainer object is then use to provide all explanations in `data_transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "data_transform = pipe.named_steps['preproc'].transform(X_test)\n",
    "\n",
    "explainer = shap.explainers.Linear(\n",
    "    pipe.named_steps['clf'], \n",
    "    data_transform,\n",
    "    feature_names = pipe.named_steps['preproc'].get_feature_names_out())\n",
    "\n",
    "shap_values = explainer(data_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waterfall Plots\n",
    "\n",
    "From [SHAP's documentation](https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/waterfall.html) (emphasis added):\n",
    "\n",
    "\n",
    "> Waterfall plots are designed to display explanations for **individual predictions**, so they expect a single row of an Explanation object as input. The bottom of a waterfall plot starts as the expected value of the model output, and then each row shows how the positive (red) or negative (blue) contribution of each feature moves the value from the expected model output over the background dataset to the model output for this prediction.\n",
    "\n",
    "+ The waterfall plot below shows the contribution of each feature to an individual prediction. \n",
    "+ Only the most important features are shown, while the least important features are grouped together at the bottom of the chart (e.g., \"4 other features\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beeswarm plot\n",
    "\n",
    "+ Beeswarm plots display the contributions of each feature to all cases in the sample. The feature values are color-coded when available. \n",
    "+ Beeswarm plots summarize the behaviour of the model across all items in the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_production",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
